name: CI

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]

# Restrict permissions for security
permissions:
  contents: read

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    strategy:
      matrix:
        node-version: [18.x, 20.x, 22.x]
      fail-fast: false

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Use Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install build dependencies for native modules
        run: sudo apt-get update && sudo apt-get install -y python3 make g++

      - name: Install dependencies
        run: npm ci

      - name: Check formatting
        run: npm run format:check

      - name: Lint
        run: npm run lint

      - name: Build
        run: npm run build

      - name: Run tests
        run: npm test

      - name: Run tests with coverage
        if: matrix.node-version == '20.x'
        run: npm run test:coverage

      - name: Upload coverage to Codecov
        if: matrix.node-version == '20.x'
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage/lcov.info
          fail_ci_if_error: false
        env:
          CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}

  # Test with real Ollama LLM (full production model: llama3.1:8b)
  # Note: This is slow on GitHub runners (no GPU) but tests the real AI
  ollama-test:
    runs-on: ubuntu-latest
    timeout-minutes: 25
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'

      - name: Install Ollama
        run: curl -fsSL https://ollama.com/install.sh | sh

      - name: Start Ollama server
        run: |
          ollama serve &
          sleep 5

      - name: Pull model (full production model)
        run: ollama pull llama3.1:8b

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Run Ollama integration tests
        run: npm run test:ollama
        timeout-minutes: 15

  # Build Docker image
  docker-build:
    runs-on: ubuntu-latest
    needs: build-and-test
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build Docker image
        run: docker build -t nexi-ai:test .

      - name: Test Docker image
        run: |
          docker run --rm nexi-ai:test node -e "console.log('Docker image built successfully')"

  security:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Use Node.js 20.x
        uses: actions/setup-node@v4
        with:
          node-version: 20.x
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run security audit
        run: npm audit --audit-level=high
        continue-on-error: true
